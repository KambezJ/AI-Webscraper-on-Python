# AI Web Scraper with Streamlit

## Overview
This project is an AI-powered web scraper that uses Python and Streamlit to scrape web content, extract relevant text, and parse specific information using natural language prompts. The tool leverages Selenium for web scraping, BeautifulSoup for HTML parsing, and LangChain's Ollama integration for AI-driven content extraction.

## Features

### Core Features
1. **Dynamic Web Scraping**:
   - Scrapes web pages using Selenium, fetching the full DOM (Document Object Model).

2. **Text Content Extraction**:
   - Extracts and cleans the `<body>` content of web pages, removing scripts and styles for a streamlined output.

3. **AI-Powered Parsing**:
   - Uses natural language prompts to extract specific information from the scraped content via the Ollama LLM integration.

4. **User-Friendly Interface**:
   - Built with Streamlit for a clean, interactive interface that allows users to input URLs, view DOM content, and describe what they want to parse.

5. **Modular Codebase**:
   - Clear separation of scraping, cleaning, and parsing logic across multiple Python files for maintainability.

### Additional Features
- **Expandable DOM Content Viewer**:
  - Users can view the scraped DOM content in an expandable text area.
- **Chunked Parsing**:
  - Large DOM content is split into manageable chunks to avoid processing limitations.

## How It Works

### 1. Scraping the Website
The `scrape_website` function in `scrape.py` uses Selenium to load the web page and fetch the page source, including dynamic content generated by JavaScript.

### 2. Extracting and Cleaning Content
- **`extract_body_content`:** Extracts the `<body>` section of the HTML.
- **`clean_body_content`:** Removes unnecessary tags and content (e.g., scripts, styles) and converts the DOM to readable text.

### 3. Parsing Content with AI
The cleaned content is split into smaller chunks using `split_dom_content`, and each chunk is processed by the `parse_with_ollama` function in `parse.py`. This function:
- Generates a prompt based on the user's description.
- Uses the LangChain Ollama integration to extract the requested information.

### 4. Streamlit Interface
- Users enter a URL and describe the information they want to extract.
- Results are displayed interactively, with options to view intermediate outputs like cleaned DOM content.

## Screenshots
*Include screenshots of the Streamlit interface and example outputs.*

## Getting Started

### Prerequisites
- Python 3.9 or later
- **Virtual Environment**:
  ```bash
  python -m venv ai
  source ai/bin/activate  # On Windows, use ai\Scripts\activate
  ```

### Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/ai-web-scraper.git
   cd ai-web-scraper
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Download the appropriate version of `chromedriver` and place it in the project root directory.

### Running the Application
1. Activate the virtual environment:
   ```bash
   source ai/bin/activate
   ```
2. Run the Streamlit application:
   ```bash
   streamlit run main.py
   ```

3. Enter a website URL in the Streamlit interface, describe the data you want to extract, and view the results interactively.

## Code Overview

### 1. `main.py`
- **Streamlit Frontend**:
  - Handles user input for URL and parsing description.
  - Displays intermediate outputs like cleaned DOM content and final parsed results.
- **Session State**:
  - Stores cleaned DOM content between interactions.

### 2. `scrape.py`
- **`scrape_website`**:
  - Uses Selenium to load a website and fetch its page source.
- **`extract_body_content`**:
  - Extracts the `<body>` section of the HTML.
- **`clean_body_content`**:
  - Removes unnecessary tags and converts the body content into readable text.
- **`split_dom_content`**:
  - Splits large content into smaller chunks for easier processing.

### 3. `parse.py`
- **LangChain Ollama Integration**:
  - Defines a custom prompt for parsing specific information.
  - Uses the Ollama LLM (`llama3.1`) to extract information from each chunk of content.
- **`parse_with_ollama`**:
  - Iterates over content chunks, sending each to the LLM for processing.

### 4. `requirements.txt`
Lists all necessary dependencies, including:
- `streamlit`: For the frontend interface.
- `langchain`, `langchain_ollama`: For AI-driven parsing.
- `selenium`, `beautifulsoup4`: For web scraping and HTML parsing.
- `lxml`, `html5lib`: Additional parsing tools.

## Future Enhancements

1. **Captcha Handling**:
   - Integrate BrightData or a similar service for captcha-solving and proxy management.
2. **File Output**:
   - Save parsed results to a downloadable file (e.g., CSV or JSON).
3. **Error Handling**:
   - Add robust error messages for invalid URLs, timeouts, or unsupported pages.
4. **Expanded AI Parsing**:
   - Allow users to select different AI models or adjust parsing parameters.
5. **Progress Indicators**:
   - Show progress bars for scraping and parsing processes.

## Skills Demonstrated

### Core Python Skills
1. **Web Scraping**:
   - Using Selenium to handle dynamic websites.
   - Parsing and cleaning HTML with BeautifulSoup.
2. **Streamlit Interface**:
   - Building an interactive frontend for user input and results display.
3. **AI Integration**:
   - Leveraging LangChain's Ollama integration for natural language parsing.

### Additional Skills
- **Asynchronous Processing**:
  - Chunking large content and processing it iteratively.
- **Session State Management**:
  - Maintaining user data across interactions in Streamlit.
- **Modular Programming**:
  - Organizing code into reusable functions and modules.

## Acknowledgements
- Selenium WebDriver documentation: [https://www.selenium.dev/]
- LangChain and Ollama integration: [https://docs.langchain.com/]
- BeautifulSoup documentation: [https://www.crummy.com/software/BeautifulSoup/]

